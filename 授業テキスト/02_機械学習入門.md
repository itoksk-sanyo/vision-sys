# 第2時：ディープラーニングで物体認識を体験しよう

## 🎯 今日の目標
AIがどうやって画像を認識するのか理解して、実際に体験する！

---

## 📝 機械学習とディープラーニング

### 人間の学習 vs 機械の学習

**人間の場合**
1. 猫を見る → 「これは猫だ」と教わる
2. 何度も見る → 特徴を覚える（耳の形、ヒゲ、鳴き声）
3. 新しい猫を見る → 「これも猫だ！」と分かる

**機械の場合**
1. たくさんの猫の画像を見せる（データ）
2. 特徴を自動で見つける（学習）
3. 新しい画像を判定する（予測）

### ディープラーニングの仕組み

```
入力（画像） → 特徴抽出 → パターン認識 → 結果（猫/犬）
```

画像は「ピクセル（点）の集まり」として認識されます：
- 各ピクセルには色の情報（RGB）
- AIはピクセルのパターンから特徴を学習

---

## 🚀 Google Colabを使ってみよう

### ステップ1：Google Colabにアクセス

1. **Google Colabを開く**
   - https://colab.research.google.com/
   - Googleアカウントでログイン

2. **新しいノートブックを作成**
   - 「ファイル」→「新しいノートブック」
   - 名前を変更：`物体認識体験.ipynb`

### ステップ2：基本操作を覚える

**セル**：コードを書く場所
- `+ コード`：新しいコードセルを追加
- `+ テキスト`：説明文を追加
- `Shift + Enter`：セルを実行

### 簡単な計算をしてみよう

```python
# これはコメント（実行されない説明文）
print("こんにちは、AI！")

# 計算もできる
2 + 3
```

実行すると：
```
こんにちは、AI！
5
```

---

## 🖼️ 事前学習済みモデルで画像認識

### VGG16モデルを使った画像認識

**VGG16とは？**
- 100万枚以上の画像で学習済み
- 1000種類の物体を認識できる
- すぐに使える優秀なモデル

### ステップ1：必要なライブラリをインストール

```python
# 画像処理と機械学習のライブラリをインストール
!pip install tensorflow pillow matplotlib
```

### ステップ2：ライブラリをインポート

```python
# 必要なライブラリを読み込む
import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
```

### ステップ3：モデルを読み込む

```python
# VGG16モデルを読み込む（初回は少し時間がかかります）
print("モデルを読み込んでいます...")
model = VGG16(weights='imagenet')
print("準備完了！")
```

### ステップ4：画像をアップロードして認識

```python
# 画像をアップロードする機能
from google.colab import files

print("認識したい画像をアップロードしてください")
uploaded = files.upload()

# アップロードされた画像のファイル名を取得
filename = list(uploaded.keys())[0]
print(f"画像 '{filename}' をアップロードしました")
```

### ステップ5：画像を前処理して予測

```python
# 画像を読み込んで前処理
img = image.load_img(filename, target_size=(224, 224))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array = preprocess_input(img_array)

# 予測実行
print("AIが画像を分析中...")
predictions = model.predict(img_array)

# 結果を解釈（上位3つ）
results = decode_predictions(predictions, top=3)[0]

# 結果を表示
print("\n=== 認識結果 ===")
for i, (imagenet_id, label, score) in enumerate(results):
    print(f"{i+1}位: {label} ({score*100:.1f}%)")
```

### ステップ6：結果を視覚的に表示

```python
# 画像と結果を並べて表示
plt.figure(figsize=(10, 5))

# 左側に画像
plt.subplot(1, 2, 1)
plt.imshow(img)
plt.title('入力画像')
plt.axis('off')

# 右側に結果のグラフ
plt.subplot(1, 2, 2)
labels = [r[1] for r in results]
scores = [r[2] for r in results]
plt.barh(labels, scores)
plt.xlabel('確率')
plt.title('認識結果 Top3')
plt.xlim(0, 1)

plt.tight_layout()
plt.show()
```

---

## 🔍 特徴マップを見てみよう

AIが画像のどこに注目しているか可視化してみましょう。

```python
# モデルの中間層を取得
from tensorflow.keras.models import Model

# 最初の畳み込み層の出力を取得
layer_outputs = [layer.output for layer in model.layers[1:6]]
activation_model = Model(inputs=model.input, outputs=layer_outputs)

# 特徴マップを計算
activations = activation_model.predict(img_array)

# 最初の層の特徴マップを表示
first_layer_activation = activations[0]
plt.figure(figsize=(15, 8))

# 8個の特徴マップを表示
for i in range(8):
    plt.subplot(2, 4, i + 1)
    plt.imshow(first_layer_activation[0, :, :, i], cmap='viridis')
    plt.title(f'特徴マップ {i+1}')
    plt.axis('off')

plt.suptitle('AIが見ている特徴（最初の層）')
plt.show()
```

---

## 🎮 練習問題

### 課題1：いろいろな画像で試してみよう

以下の画像で認識精度を確認：
1. 工具（ドライバー、レンチなど）
2. 金属部品
3. 自分のスマートフォンで撮った写真

### 課題2：認識結果をまとめよう

| 画像の内容 | 1位の予測 | 確率 | 正しかった？ |
|-----------|----------|------|-------------|
| 例：ドライバー | screwdriver | 92.3% | ○ |
| | | | |
| | | | |

### 課題3：考察

1. どんな画像が正しく認識されやすい？
2. 認識に失敗した画像の共通点は？
3. 工場で使うにはどんな改善が必要？

---

## 💡 発展課題：簡単な分類器を作ってみよう

```python
# 2つの画像を比較する関数
def compare_images(image1_path, image2_path):
    # 画像1を予測
    img1 = image.load_img(image1_path, target_size=(224, 224))
    img1_array = preprocess_input(np.expand_dims(image.img_to_array(img1), axis=0))
    pred1 = model.predict(img1_array)
    result1 = decode_predictions(pred1, top=1)[0][0]
    
    # 画像2を予測
    img2 = image.load_img(image2_path, target_size=(224, 224))
    img2_array = preprocess_input(np.expand_dims(image.img_to_array(img2), axis=0))
    pred2 = model.predict(img2_array)
    result2 = decode_predictions(pred2, top=1)[0][0]
    
    # 結果を表示
    plt.figure(figsize=(10, 5))
    
    plt.subplot(1, 2, 1)
    plt.imshow(img1)
    plt.title(f'{result1[1]}\n({result1[2]*100:.1f}%)')
    plt.axis('off')
    
    plt.subplot(1, 2, 2)
    plt.imshow(img2)
    plt.title(f'{result2[1]}\n({result2[2]*100:.1f}%)')
    plt.axis('off')
    
    plt.show()
    
    # 同じカテゴリか判定
    if result1[1] == result2[1]:
        print("✅ 同じ種類の物体です！")
    else:
        print("❌ 違う種類の物体です")
```

---

## 🤔 よくある質問

**Q: なぜ224×224のサイズにするの？**
A: VGG16モデルがその大きさで学習されたから。モデルごとに決まったサイズがあります。

**Q: 日本語で結果が出ない**
A: ImageNetは英語のデータセット。でも1000種類も認識できてすごい！

**Q: 工場の部品は認識できる？**
A: 一般的な物体は認識できるけど、特殊な部品は難しい。だから自分でモデルを作る必要があります。

---

## 📊 今日のまとめ

### 学んだこと
- [ ] 機械学習の基本的な仕組み
- [ ] Google Colabの使い方
- [ ] 画像認識の体験
- [ ] AIが見ている特徴の可視化

### 重要なポイント
1. **データが大事**：たくさんの良質なデータで学習
2. **前処理が必要**：画像のサイズや形式を統一
3. **確率で判定**：100%ではなく、可能性を示す

### 次回予告
次回は「Teachable Machine」を使って、自分だけのAIモデルを作ります！
工場で使える「良品/不良品」判定AIを作ってみましょう。

---

## 🏠 宿題

1. **実験レポート**
   - 5種類以上の画像で実験
   - 結果をまとめて提出

2. **調査課題**
   - 「Teachable Machine」について調べる
   - どんなことができそうか3つ考える

3. **準備**
   - 次回使う画像を考える（良品/不良品のサンプル）

**提出方法**: 
- Google Colabのリンクを共有
- GitHubのnotebooksフォルダにアップロード